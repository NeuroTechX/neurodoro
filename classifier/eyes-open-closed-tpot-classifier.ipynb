{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Basic EEG Classifier with TPOT\n",
    " \n",
    "1. Perform the same data wrangling techniques in the inception classifer to get panels filled with 2s epoch data\n",
    "2. Send data into a TPOT pipeline and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from tpot import TPOTClassifier  \n",
    "\n",
    "EPOCH_LENGTH = 440\n",
    "VARIANCE_THRESHOLD = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data has been collected, let's import it\n",
    "\n",
    "open_data = pd.read_csv(\"../Muse Data/DanoThursdayOpenRawEEG0.csv\", header=0, index_col=False)\n",
    "closed_data = pd.read_csv(\"../Muse Data/DanoThursdayClosedRawEEG1.csv\", header=0, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unfortunately, haven't come up with a good way to feed multi-dimensional data (i.e. including all 4 channels) into sklearn yet.\n",
    "# To get around this, we'll drop everything except Channel 1's EEG data so everything works\n",
    "\n",
    "open_array = open_data['Channel 1']\n",
    "closed_array = closed_data['Channel 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prune a few rows from the tail of these arrays so that they are all divisible by our desired epoch length\n",
    "\n",
    "open_overflow = open_array.size % EPOCH_LENGTH\n",
    "open_array = open_array[0:-open_overflow]\n",
    "closed_overflow = closed_array.size % EPOCH_LENGTH\n",
    "closed_array = closed_array[0:-closed_overflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Split DataFrames into many different dataframes 440 samples long\n",
    "np.array_split breaks apart a single array into arrays with a certain length\n",
    "in this case, it splits every 440 rows into different arrays\n",
    "np.stack puts multiple arrays on top of each other along an axis\n",
    "here it stacks all the 440-length arrays we created on top of each other as different rows in a matrix\n",
    "'''\n",
    "\n",
    "split_open_data = np.stack(np.array_split(open_array, EPOCH_LENGTH), axis=1)\n",
    "split_closed_data = np.stack(np.array_split(closed_array, EPOCH_LENGTH), axis=1)\n",
    "\n",
    "# Transform data into a 3D pandas Panel ( n epochs x 4 channels x 440 samples )\n",
    "\n",
    "open_df = pd.DataFrame(split_open_data)\n",
    "closed_df = pd.DataFrame(split_closed_data)\n",
    "open_df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 440)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove epochs with too much variance\n",
    "\n",
    "def removeNoise(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if np.var(row) > VARIANCE_THRESHOLD:\n",
    "            print('variance ', np.var(row))\n",
    "            df.drop(row)\n",
    "    return df\n",
    "\n",
    "open_df = removeNoise(open_df)\n",
    "closed_df = removeNoise(closed_df)\n",
    "closed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create a combined dataframe with both the open and closed eye data stacked on top of each other (epochs x EPOCH_LENGTH)\n",
    "The first closed_df.shape[0] rows will be 1s, indicating eyes closed, and the rest will be 0s\n",
    "'''\n",
    "\n",
    "combined_df = pd.concat([closed_df, open_df], axis=0, ignore_index=True)\n",
    "labels = np.append(np.ones(closed_df.shape[0]),np.zeros(open_df.shape[0]))\n",
    "\n",
    "# Create a sklearn train test split with this big combined df\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df, labels,  \n",
    "                                                    train_size=0.75,  \n",
    "                                                    test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a TPOTClassifier that will run for 10 generations\n",
    "\n",
    "my_tpot = TPOTClassifier(generations=10)  \n",
    "\n",
    "# Fit this baby! Takes a long time to run\n",
    "\n",
    "my_tpot.fit(X_train, y_train)  \n",
    "  \n",
    "# See what kind of score we get\n",
    "print(my_tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Holy crap! That's really good (or perhaps broken). Let's export the pipeline and see what TPOT came up with\n",
    "\n",
    "my_tpot.export('exported_pipeline.py')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what it came up with,\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)\n",
    "training_features, testing_features, training_classes, testing_classes = \\\n",
    "    train_test_split(features, tpot_data['class'], random_state=42)\n",
    "\n",
    "exported_pipeline = LinearSVC(C=25.0, dual=False, penalty=\"l1\", tol=0.1)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_classes)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some new data and test this classifier out\n",
    "\n",
    "new_open_data = pd.read_csv(\"../Muse Data/DanoEyesOpenRawEEG0.csv\", header=0, index_col=False)\n",
    "new_closed_data = pd.read_csv(\"../Muse Data/DanoEyesClosedRawEEG1.csv\", header=0, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get channel 1 data \n",
    "\n",
    "open_array = new_open_data['Channel 1']\n",
    "closed_array = new_closed_data['Channel 1']\n",
    "\n",
    "# Prune a few rows from the tail of these arrays so that they are all divisible by our desired epoch length\n",
    "\n",
    "open_overflow = open_array.size % EPOCH_LENGTH\n",
    "open_array = open_array[0:-open_overflow]\n",
    "closed_overflow = closed_array.size % EPOCH_LENGTH\n",
    "closed_array = closed_array[0:-closed_overflow]\n",
    "\n",
    "# Split into multiple arrays of EPOCH_LENGTH\n",
    "\n",
    "split_open_data = np.stack(np.array_split(open_array, EPOCH_LENGTH), axis=1)\n",
    "split_closed_data = np.stack(np.array_split(closed_array, EPOCH_LENGTH), axis=1)\n",
    "\n",
    "# Transform data into a 3D pandas Panel ( n epochs x 4 channels x 440 samples )\n",
    "\n",
    "open_df = pd.DataFrame(split_open_data)\n",
    "closed_df = pd.DataFrame(split_closed_data)\n",
    "\n",
    "# Remove noise\n",
    "open_df = removeNoise(open_df)\n",
    "closed_df = removeNoise(closed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_combined_df = pd.concat([closed_df, open_df], axis=0, ignore_index=True)\n",
    "new_labels = np.append(np.ones(closed_df.shape[0]),np.zeros(open_df.shape[0]))\n",
    "\n",
    "# Create a sklearn train test split with this big combined df\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_combined_df, new_labels,  \n",
    "                                                    train_size=0.75,  \n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're doing this by hand in the notebook, I'll just use the meaty parts of the exported pipeline\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "training_features, testing_features, training_classes, testing_classes = \\\n",
    "    train_test_split(new_combined_df, new_labels, random_state=42)\n",
    "\n",
    "exported_pipeline = LinearSVC(C=25.0, dual=False, penalty=\"l1\", tol=0.1)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_classes)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the accuracy of this guy\n",
    "\n",
    "exported_pipeline.score(testing_features, testing_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welp, there it is again"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
